{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import string\n",
    "import json\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "products = pd.read_csv('amazon_baby.csv')\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform text cleaning\n",
    "def remove_punctuation(text):\n",
    "    trans = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(trans) \n",
    "\n",
    "products.review.fillna('', inplace=True)\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract sentiment\n",
    "products = products[products.rating != 3]\n",
    "\n",
    "products['sentiment'] = products['rating'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "products.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train test data\n",
    "\n",
    "# getting the tarin indices\n",
    "with open('module-2-assignment-train-idx.json') as f:\n",
    "    train_idx = json.load(f)\n",
    "    \n",
    "# getting the test indices\n",
    "with open('module-2-assignment-test-idx.json') as f:\n",
    "    test_idx = json.load(f)\n",
    "    \n",
    "train_data = products.iloc[train_idx]\n",
    "test_data = products.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the word count vector for each review\n",
    "\n",
    "vectorized = CountVectorizer(token_pattern=r'\\b\\w+\\b') # This token pattern to keep single-letter words\n",
    "\n",
    "# train using train dataset\n",
    "train_matrix = vectorized.fit_transform(train_data['review_clean'])\n",
    "\n",
    "# convert the test dataset\n",
    "test_matrix = vectorized.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model\n",
    "sentiment_model = LogisticRegression()\n",
    "sentiment_model.fit(train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many wiehgts are >= 0?\n",
    "\n",
    "coeff_array = sentiment_model.coef_\n",
    "print((coeff_array >= 0).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making perdictions with the logistic regression\n",
    "sample_test_data = test_data[10:13]\n",
    "print(sample_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# digging in the 1st row of the sampel_test_data\n",
    "sample_test_data.iloc[0]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at the next row (-ve review)\n",
    "sample_test_data.iloc[1]['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the score for the sample_test_data\n",
    "sample_test_matrix = vectorized.transform(sample_test_data['review_clean'])\n",
    "scores = sentiment_model.decision_function(sample_test_matrix)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict sentiment\n",
    "sentiment_model.predict(sample_test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probablity predictions\n",
    "probability = 1/(1 + np.exp(-scores))\n",
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the most positive (and negative) review\n",
    "\n",
    "test_matrix = vectorized.transform(test_data['review_clean'])\n",
    "scores_test = sentiment_model.decision_function(test_matrix)\n",
    "predictions = 1/(1+np.exp(-scores_test))\n",
    "\n",
    "test_data['predictions'] = predictions\n",
    "\n",
    "test_data.sort_values('predictions', ascending=False).iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.sort_values('predictions', ascending = True).iloc[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the accuracy of the classifier\n",
    "predicted_sentiment = sentiment_model.predict(test_matrix)\n",
    "test_data['predicted_sentiment'] = predicted_sentiment\n",
    "test_data['diff_sentiment'] = predicted_sentiment - test_data['sentiment']\n",
    "acc = np.sum(sum(test_data['diff_sentiment'] == 0))\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(test_data.index)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(float(acc)/float(total), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn another classifer with fewer words\n",
    "\n",
    "significant_words =['love', 'great', 'easy', 'old', 'little', 'perfect', 'loves', \n",
    "      'well', 'able', 'car', 'broke', 'less', 'even', 'waste', 'disappointed', \n",
    "      'work', 'product', 'money', 'would', 'return']\n",
    "\n",
    "vectorizer_word_subset = CountVectorizer(vocabulary=significant_words) # limit to 20 words\n",
    "train_matrix_word_subset = vectorizer_word_subset.fit_transform(train_data['review_clean'])\n",
    "test_matrix_word_subset = vectorizer_word_subset.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on a subset of data\n",
    "\n",
    "simple_model = LogisticRegression()\n",
    "simple_model.fit(train_matrix_word_subset, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_coef_table = pd.DataFrame({'word':significant_words,\n",
    "                                        'coefficient':simple_model.coef_.flatten()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model_coef_table.sort_values('coefficient', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(simple_model_coef_table['coefficient']>=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vectorized.vocabulary_.keys())\n",
    "coeffs = {vocab[i]: c for i, c in enumerate(sentiment_model.coef_[0])}\n",
    "new_dic = {k:v for k, v in coeffs.items() if k in significant_words}\n",
    "new_table = pd.DataFrame(new_dic.items(), columns=['word', 'new_coeffi'])\n",
    "new_table_coeff = pd.merge(simple_model_coef_table, new_table, how = 'left', on = 'word' )\n",
    "new_table_coeff = new_table_coeff[new_table_coeff['coefficient']>=0]\n",
    "sum(new_table_coeff['new_coeffi'] < 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_table_coeff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing models\n",
    "\n",
    "predicted_sentiment_train_sentiment = sentiment_model.predict(train_matrix)\n",
    "train_data['predicted_sentiment_ts'] = predicted_sentiment_train_sentiment\n",
    "acc_ts = round(float(sum(train_data['predicted_sentiment_ts'] == train_data['sentiment']))/len(train_data.index),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_simple_train_sentiment = simple_model.predict(train_matrix_word_subset)\n",
    "train_data['predicted_simple_ts'] = predicted_simple_train_sentiment\n",
    "acc_tsimple = round(float(sum(train_data['predicted_simple_ts'] == train_data['sentiment']))/len(train_data.index),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ts > acc_tsimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_tsimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sentiment_test_sentiment = sentiment_model.predict(test_matrix)\n",
    "test_data['predicted_sentiment_ts'] = predicted_sentiment_test_sentiment\n",
    "acc_ts_test = round(float(sum(test_data['predicted_sentiment_ts'] == test_data['sentiment']))/len(test_data.index),2)\n",
    "acc_ts_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_simple_test_sentiment = simple_model.predict(test_matrix_word_subset)\n",
    "test_data['predicted_simple_ts'] = predicted_simple_test_sentiment\n",
    "acc_tsimple_test = round(float(sum(test_data['predicted_simple_ts'] == test_data['sentiment']))/len(test_data.index),2)\n",
    "acc_tsimple_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_ts_test > acc_tsimple_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the majority class model\n",
    "\n",
    "sum(train_data['sentiment'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(train_data['sentiment'] == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(float(sum(test_data['sentiment'] ==1))/len(test_data.index),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
